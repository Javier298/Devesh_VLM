{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b6e80d-04b9-4818-9e24-f66895b815da",
   "metadata": {},
   "source": [
    "# **Inferencing Qwen2-VL-2B-Instruct on dacl_test_dev**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f551bbe6-5cb8-4253-b4c5-d7e05937454c",
   "metadata": {},
   "source": [
    "## I will be inferencing the VLM on dacl-10k test dataset, and generate an inspection report in pdf format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b9d75-ab7e-4983-8c3c-db62043fa859",
   "metadata": {},
   "source": [
    "#### The check-point shards of qwen are already downloaded. TO AVOID DOWNLOADING STUFF AGAIN, I have written the below line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5539b5bf-3a67-4866-9e49-834cddc06b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######TO AVOID DWONLOADING STUFF (SHARDS) AGAIN#######################################################################\n",
    "import os\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING']= \"1\"\n",
    "#os.environ['TRANSFORMERS_CACHE'] = 'D:/mdfBIM+ - VLM 4 Bridge Damages - Jäkel_Bitte nicht löschen!_/hugging_face/hub'\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'C:/Users/Ddimble/.cache/huggingface/hub' #if the checkpoint shards are not downloaded, then comment this line "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466087f6-099b-4edf-bc0b-17eff3966451",
   "metadata": {},
   "source": [
    "## IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e2ef6a-ec56-44a4-bf17-24667b94fd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################IMPORT LIBRARIES###################################\n",
    "#os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor, BitsAndBytesConfig, Trainer, TrainingArguments, get_scheduler\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft.optimizers import create_loraplus_optimizer\n",
    "import bitsandbytes as bnb\n",
    "import wandb\n",
    "import warnings\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Spacer, Paragraph\n",
    "from reportlab.platypus import Image as replIm\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from datetime import date\n",
    "from qwen_vl_utils import process_vision_info\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import ast\n",
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e17ec6d-5a1d-4749-b4bf-8dce3b010c63",
   "metadata": {},
   "source": [
    "## Clearing memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a2764-1770-47f4-b19a-0abae267f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################CLEAR MEMORY############################################\n",
    "\n",
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if \"inputs\" in globals():\n",
    "        del globals()[\"inputs\"]\n",
    "    if \"model\" in globals():\n",
    "        del globals()[\"model\"]\n",
    "    if \"processor\" in globals():\n",
    "        del globals()[\"processor\"]\n",
    "    if \"trainer\" in globals():\n",
    "        del globals()[\"trainer\"]\n",
    "    if \"peft_model\" in globals():\n",
    "        del globals()[\"peft_model\"]\n",
    "    if \"bnb_config\" in globals():\n",
    "        del globals()[\"bnb_config\"]\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c22764-f347-4c24-aa14-8dba8c415383",
   "metadata": {},
   "source": [
    "## Resizing the images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692d90dd-af2f-4bbd-97f3-f27dd99cf50f",
   "metadata": {},
   "source": [
    "#### The model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation.\n",
    "\n",
    "https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b259e586-d11c-4ce5-8acd-a3a688a6b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################RESIZING THE IMAGE###########################################################################\n",
    "def resize_image(image: Image.Image):\n",
    "    min_size = 56  # Ensure minimum 56X56\n",
    "    max_size = 1260#1008 #Ensure max 1008X1008 #1120\n",
    "    width, height = image.size\n",
    "    \n",
    "    if width < min_size or height < min_size:\n",
    "        image = image.resize((max(width,min_size), max(height,min_size)), Image.BILINEAR)\n",
    "    elif width > max_size or height > max_size:\n",
    "        image = image.resize((min(width,max_size), min(height,max_size)), Image.BILINEAR)\n",
    "    return image\n",
    "\n",
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ac91b2-1253-4da1-8a2d-6a847c85ad89",
   "metadata": {},
   "source": [
    "## Mappings from numbers to damage and object types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3940a8c-c16f-467f-a2ed-eb3ff8945f9e",
   "metadata": {},
   "source": [
    "Numbers from 12 to 17 (both included) are object types. Rest are damage types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd3220-a57f-4488-be57-14b4cf73f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################MAPPINGS######################################################\n",
    "label_mapping = {\n",
    "                                    0: \"Crack\",\n",
    "                                    1: \"ACrack\",\n",
    "                                    2: \"Wetspot\",\n",
    "                                    3: \"Efflorescence\",\n",
    "                                    4: \"Rust\",\n",
    "                                    5: \"Rockpocket\",\n",
    "                                    6: \"Hollowareas\",\n",
    "                                    7: \"Cavity\",\n",
    "                                    8: \"Spalling\",\n",
    "                                    9: \"Graffiti\",\n",
    "                                    10: \"Weathering\",\n",
    "                                    11: \"Restformwork\",\n",
    "                                    12: \"ExposedRebars\",\n",
    "                                    13: \"Bearing\",\n",
    "                                    14: \"EJoint (Expansion Joint)\",\n",
    "                                    15: \"Drainage\",\n",
    "                                    16: \"PEquipment (Protective Equipment)\",\n",
    "                                    17: \"JTape (Joint Tape)\",\n",
    "                                    18: \"Concrete Corrosion (ConcreteC)\",\n",
    "                                    19: \"Corrosion, no rust staining\",\n",
    "                                    20: \"NO Exposed Reinforcement\",\n",
    "                                    21: \"Scaling\",\n",
    "                                    22: \"General Defects\",\n",
    "                                    23: \"No defect\"\n",
    "                                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e69d7-90e8-42ce-bd13-b3a0dd357805",
   "metadata": {},
   "source": [
    "## IMAGE FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674ed43-545d-4683-be3d-b00ddb389b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################IMAGE FOLDER#########################################################\n",
    "image_folder = \"./datasets/dacl10k_v2_devphase/images/testdev/\" #modify it accordingly\n",
    "image_name = \"Report Example 05.jpg\" #\"Report Example 05.jpg\" #in-case of #\"dacl10k_v2_testdev_0580.jpg\" ,just change the last 3 digits\n",
    "image_path = os.path.join(image_folder, image_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e522f1-2291-473d-8abd-95c54a59556f",
   "metadata": {},
   "source": [
    "### **PLEASE NOTE!!**\n",
    "\n",
    "**Here, I have split this task into 2 parts. Since the model is fine-tuned just on damage types, I will first ask it to identify the damage, and then feed this output to the model itself and ask it to generate rest of the content of the inspection report.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c194ae8c-9726-463b-b686-46b85a58f7af",
   "metadata": {},
   "source": [
    "## Conversation part 1\n",
    "\n",
    "#### Identify the damage types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab34469-5756-40be-b14b-21da6ccf84f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################CONVERSATION PART 1#######################################################\n",
    "system_message = \"\"\"You are a highly advanced Vision Language Model (VLM), specialized in analyzing, describing, and interpreting visual data. \n",
    "You have currently learned about several bridge-damage types. Your task is to generate a short inspection report on seeing the image.\"\"\"\n",
    "conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": resize_image(Image.open(image_path)) #Image.open(image_path) #resize_image(Image.open(image_path)),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"\"\"Here is the label-mapping of numbers to damage types {label_mapping}. Numbers 12 to 17(both included) are object types. Using the numbers, state the damage type(s) and object type(s) present in the image:\"\"\"\n",
    "                 },\n",
    "            ],\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89711d9-a3fd-46df-9e77-5c6c823eeaae",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7d18b-3d10-4290-8bbf-c7f4e019a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################LOAD FINE-TUNED MODEL#######################################################\n",
    "clear_memory()\n",
    "model_id = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "     \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "     torch_dtype= torch.bfloat16, #torch.bfloat16,\n",
    "     attn_implementation=\"flash_attention_2\",\n",
    "     #load_in_4bit=True,\n",
    "     low_cpu_mem_usage=True,\n",
    "     #quantization_config=bnb_config,\n",
    "     device_map=\"auto\",\n",
    "     use_cache=False,\n",
    " )\n",
    "\n",
    "#device = \"cuda\"\n",
    "#model.to(\"cpu\")\n",
    "\n",
    "min_pixels = 4 * 28 * 28\n",
    "max_pixels = 2025 * 28 * 28 #1296 * 28 * 28\n",
    "processor = Qwen2VLProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "print(f\"Before adapter parameters: {model.num_parameters()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918732c3-374f-4b5e-8e58-84e6a534d84a",
   "metadata": {},
   "source": [
    "## Load the adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5441ac98-dec7-47fa-a55d-5d0173a988b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path_1 = \"./fine_tuned_weights/codebrim_with_metrics\" #\"./output\"\n",
    "adapter_path_2 = \"./fine_tuned_weights/2601506_with_metrics\"\n",
    "adapter_path_3 = \"./fine_tuned_weights/dacl_with_metrics\"\n",
    "\n",
    "'''\n",
    "weighted_adapter_name=\"codebrim-2601506-dacl\" #adapter_path_1\n",
    "peft_model = PeftModel.from_pretrained(model,adapter_path_1, adapter_name=\"codebrim\")\n",
    "#peft_model.load_adapter(adapter_path_2, adapter_name=\"2601506\")\n",
    "peft_model.load_adapter(adapter_path_3, adapter_name=\"dacl\")\n",
    "\n",
    "peft_model.add_weighted_adapter(adapters=[\"codebrim\", \"dacl\"], weights=[0.28,0.72], adapter_name=weighted_adapter_name,combination_type=\"linear\") #\"codebrim\"\n",
    "peft_model.set_adapter(weighted_adapter_name)\n",
    "'''\n",
    "\n",
    "print(f\"Before adapter parameters: {model.num_parameters()}\")\n",
    "#'''\n",
    "adapter_name =\"dacl\"\n",
    "peft_model = PeftModel.from_pretrained(model,adapter_path_3, adapter_name=adapter_name)\n",
    "#model.load_adapter(adapter_path_3)\n",
    "#'''\n",
    "\n",
    "print(f\"After adapter parameters: {model.num_parameters()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab21f1bc-1c96-4e85-a98b-254b8a352adf",
   "metadata": {},
   "source": [
    "## Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d5dc04-4290-452f-8cba-baab8d3a85ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the inputs\n",
    "text_prompt = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "#print(f\"Before adapter parameters: {model.num_parameters()}\")\n",
    "\n",
    "image_inputs,_ = process_vision_info(conversation)\n",
    "\n",
    "inputs = processor(\n",
    "    text=[text_prompt], images=image_inputs, padding=True, return_tensors=\"pt\"\n",
    ")\n",
    "inputs = inputs.to(\"cuda\") #\"cuda\"\n",
    "\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "    output = model(**inputs)\n",
    "    #print(output)\n",
    "\"\"\"\n",
    "\n",
    "# Inference: Generation of the output\n",
    "output_ids = model.generate(**inputs, max_new_tokens=2048) #.to(\"cpu\")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids) :]\n",
    "    for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "new_output = ast.literal_eval(output_text[0])\n",
    "output_labels = [label_mapping.get(int(op)) for op in new_output] ## converting numbers to labels using label_mapping dict\n",
    "\n",
    "\n",
    "damage_str = \", \".join(output_labels) ## this gives a string \n",
    "\n",
    "print(f\"Current number of parameters: {model.num_parameters()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1cb8eb-ecdf-4bdd-978c-b15ecc5facd7",
   "metadata": {},
   "source": [
    "## Conversation Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2052339-b422-4073-a548-c93a0eda01b1",
   "metadata": {},
   "source": [
    "### Generate rest of the report content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fb091f-7500-48c8-a58f-352497472649",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################CONVERSATION PART 2##########################################\n",
    "conversation2 = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": resize_image(Image.open(image_path)) #Image.open(image_path) #resize_image(Image.open(image_path)), #val_dataset3[69][1][\"content\"][0][\"image\"]\n",
    "            },\n",
    "            {\"type\": \"text\", \n",
    "             \"text\": f\"\"\"Here is the label-mapping of numbers to damaage types {label_mapping}. Based on identified damage type(s): {output_text[0]}, give a concise report containing the following details:\\n\n",
    "                    - Damages and object type(s): {damage_str}\\n\n",
    "                    - Impact: (Brief description of the effect on the structure)\\n\n",
    "                    - Size: (Estimated size in cm² if possible)\\n\n",
    "                    - Direction: (Horizontal, vertical, diagonal, etc.)\\n\n",
    "                    - Possible Reasons: (What could have caused this damage?)\\n:\"\"\"\n",
    "                    }, #Numbers 12 to 17(both included) are object types.\n",
    "                    #Give a short inspection report consisting of :  1. damage impact, 2. damage size (in cm²), 3. damage direction, 4. possible causes #, \"\"object type\"\", and \"\"its functonalities\"\"\n",
    "        ],\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2875fc-fe9a-4d8d-9f95-b139d7837ef9",
   "metadata": {},
   "source": [
    "## Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50641d3-bdf3-42ee-9288-412120aad294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the inputs\n",
    "text_prompt2 = processor.apply_chat_template(conversation2, tokenize=False, add_generation_prompt=True)\n",
    "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "#print(f\"Before adapter parameters: {model.num_parameters()}\")\n",
    "\n",
    "image_inputs2,_ = process_vision_info(conversation2)\n",
    "#print(image_inputs)\n",
    "inputs2 = processor(\n",
    "    text=[text_prompt2], images=image_inputs2, padding=True, return_tensors=\"pt\"\n",
    ")\n",
    "inputs2 = inputs2.to(\"cuda\") #\"cuda\"\n",
    "\n",
    "# Inference: Generation of the output\n",
    "output_ids2 = model.generate(**inputs2, max_new_tokens=2048) #8192\n",
    "generated_ids2 = [\n",
    "    output_ids2[len(input_ids2) :]\n",
    "    for input_ids2, output_ids2 in zip(inputs2.input_ids, output_ids2)\n",
    "]\n",
    "output_text2 = processor.batch_decode(\n",
    "    generated_ids2, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "#print(f\"Inspection Report: \\n{output_text2[0]}\")\n",
    "\n",
    "peft_model.unload()\n",
    "peft_model.delete_adapter(adapter_name) #weighted_adapter_name #adapter_name\n",
    "print(f\"Current number of parameters: {model.num_parameters()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb67ef8-12b6-436d-88ff-2626c4dd311c",
   "metadata": {},
   "source": [
    "### Now we will generate the inspection report in pdf format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9acc97f-bb19-4761-bfcc-3ee3b2a35efe",
   "metadata": {},
   "source": [
    "## Storing the report content in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6d0110-4942-4c02-af14-158bc61bce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################STORING THE CONTENT IN A DICTIONARY##############################################\n",
    "damage_info = {}\n",
    "current_key = None  # To track the current dictionary key\n",
    "\n",
    "for line in output_text2[0].strip().split(\"\\n\"):\n",
    "    line = line.strip()\n",
    "    \n",
    "    if line.startswith(\"- \"):  \n",
    "        # New key-value pair\n",
    "        key, value = line[2:].split(\": \", 1)  # Remove \"- \" and split at \": \"\n",
    "        current_key = key.strip()\n",
    "        damage_info[current_key] = value.strip()\n",
    "    elif current_key:\n",
    "        # Continuation of the previous value\n",
    "        damage_info[current_key] += \" \" + line.strip()\n",
    "\n",
    "\n",
    "#print(damage_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c6400c-355e-4334-b5db-7e8c75b43a6f",
   "metadata": {},
   "source": [
    "## create_pdf function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1edf8be-8ab7-4091-a62f-6530ae1da41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################CREATING A PDF#########################################\n",
    "def create_pdf(image_name, details):\n",
    "    base_name = os.path.splitext(image_name)[0]\n",
    "    output_pdf = f\"{base_name}.pdf\"\n",
    "\n",
    "    # Create the PDF document\n",
    "    doc = SimpleDocTemplate(output_pdf, pagesize=A4)\n",
    "    #elements = []  # Holds all the components (image + table)\n",
    "\n",
    "    # Add the image\n",
    "    img = replIm(image_path, width=300, height=200) #Image(image_path, width=300, height=200)  \n",
    "    #elements.append(img)\n",
    "\n",
    "    styles = getSampleStyleSheet()\n",
    "    text_style = styles[\"BodyText\"] \n",
    "    # Define table data\n",
    "    data = [\n",
    "        [\"Category\", \"Details\"],  # Table headers\n",
    "        [\"Project Name\", \"x\"],\n",
    "        [\"Project ID\", \"x\"],\n",
    "        [\"Project Location\", \"x\"],\n",
    "        [\"Company Name\", \"ICoM GmbH\"],\n",
    "        [\"Inspector Name\", \"Max Mastermann\"],\n",
    "        [\"Date of Inspection\", f\"{date.today().strftime(\"%d/%m/%Y\")}\"],\n",
    "        [\"Damage & Object Type(s)\", Paragraph(damage_str)],\n",
    "        [\"Impact\", Paragraph(details[\"Impact\"])],\n",
    "        [\"Size\", Paragraph(details[\"Size\"])],\n",
    "        [\"Direction\", Paragraph(details[\"Direction\"])],\n",
    "        [\"Possible Reasons\", Paragraph(details[\"Possible Reasons\"])],\n",
    "    ]\n",
    "\n",
    "    # Create the table\n",
    "    table = Table(data, colWidths=[2.5 * inch, 4 * inch]) #[150, 350]\n",
    "\n",
    "    # Table styling\n",
    "    style = TableStyle([\n",
    "        (\"BACKGROUND\", (0, 0), (-1, 0), colors.grey),  # Header background\n",
    "        (\"TEXTCOLOR\", (0, 0), (-1, 0), colors.white),  # Header text color\n",
    "        (\"ALIGN\", (0, 0), (-1, -1), \"LEFT\"),  # Align all text to left\n",
    "        (\"FONTNAME\", (0, 0), (-1, 0), \"Helvetica-Bold\"),  # Header font\n",
    "        (\"BOTTOMPADDING\", (0, 0), (-1, 0), 10),  # Header padding\n",
    "        (\"GRID\", (0, 0), (-1, -1), 1, colors.black),  # Add grid lines\n",
    "    ])\n",
    "\n",
    "    table.setStyle(style)\n",
    "\n",
    "    #elements.append(table)  # Add table to document\n",
    "\n",
    "    # Build the PDF\n",
    "    #doc.build(elements)\n",
    "    doc.build([img, Spacer(1,20), table])\n",
    "    print(f\"PDF saved as {output_pdf}\")\n",
    "################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed7e030-ccfa-42db-886b-9fa5c8ee9b06",
   "metadata": {},
   "source": [
    "## call the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791033a1-292a-45d1-beda-904ea9091884",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_name = os.path.splitext(image_name)[0]\n",
    "#output_pdf = f\"{base_name}.pdf\"\n",
    "text = f\"Bridge Damage Report\\n\\n output_text2[0]\" #not used anywhere\n",
    "create_pdf(image_name, damage_info)\n",
    "#print(f\"PDF saved as {output_pdf}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
